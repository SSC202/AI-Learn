# AL_Learn 4_线性神经网络

## 1. 线性回归

**回归（regression）**是能为一个或多个自变量与因变量之间关系建模的一类方法。

### 线性回归的基本要素

1. 基本假设：

> - 自变量$x$和因变量$y$之间的关系是线性的。（$y$可以表示为$x$的加权和）（允许少量的噪声存在）
> - 假设任何噪声都比较正常，如噪声遵循正态分布。

2. 线性回归的基本要素

**训练数据集（training set）：**用于进行模型训练；

**样本（sample）**：训练数据集的每行数据；

**标签（label）**：试图预测的目标；

**特征（feature）**：预测依据的自变量。

3. 通常的，使用$n$表示数据集中的样本数，对于索引为$i$的样本，输入为$x^{(i)} = [x_1^{(i)},x_2^{(i)}]$，输出为$y^{(i)}$。

### 线性回归的基本模型

**线性假设指目标和特征的加权和。**
$$
\hat{y} = w_1x_1 + w_2x_2 + \ldots + w_mx_m + b
$$
式中$w_j$为**权重（weight）**，权重决定了每个特征对预测值的影响。$b$为**偏置（bias）**，指当所有特征都取值为0时，预测值应该为多少。

上式也被称之为输入特征的**仿射变换**，仿射变换的特点是通过加权和对特征进行**线性变换（linear transformation）**， 并通过偏置项来进行**平移（translation）**。

给定一个数据集，目标为寻找合适的权重和偏置，使得模型的预测尽可能准确。输出的预测值由输入特征通过线性模型的仿射变换决定，仿射变换由所选权重和偏置确定。

当输入包含$m$个特征时，令特征向量$\bold{x} \in \mathbb{R}^m $，权重向量$\bold{w} \in \mathbb{R}^b$，则
$$
\hat{y} = \bold{w}^T\bold{x} + b
$$
使用矩阵$\bold{X} \in \mathbb{R}^{n \times m}$可以表示每个样本。$\bold{X}$的每一行为一个样本，每一列为一个特征。则预测值：
$$
\bold{\hat{y}} = \bold{Xw}+b
$$
则目标为，给定训练数据特征$\bold{X}$和对应已知标签$\bold{y}$，找到一组权重向量$\bold{w}$和偏置$b$，使得预测误差尽可能小。

即使确信特征与标签的潜在关系是线性的， 也应加入一个噪声项来考虑观测误差带来的影响。

### 线性回归的损失函数

*损失函数*（loss function）能够量化目标的实际值与预测值之间的差距。 通常选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。 

回归问题中最常用的损失函数是平方误差函数。 当样本$i$的预测值为$\hat{y}^{(i)}$，其相应的真实标签为$y^{(i)}$时， 平方误差可以定义为以下公式：
$$
l^{(i)}(\bold{w},b)=\frac{1}{2}(\hat{y}^{(i)}-y^{(i)})^2
$$
 由于训练数据集并不受控制，所以**经验误差只是关于模型参数的函数**。

对于$n$个样本而言，使用均值衡量总体的损失。
$$
L(\bold{w},b) = \frac{1}{n}\sum^n_{i=1}l^{(i)}(\bold{w},b)=\frac{1}{n}\sum^n_{i=1}\frac{1}{2}(\bold{w}^T\bold{x}^{(i)}+b-y^{(i)})^2
$$
训练模型时，我们希望寻找一组参数$(\bold{w}^{*},b^*)$使得：
$$
\bold{w}^{*},b^* = argmin \quad L(\bold{w},b)
$$
线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解。

> 首先，将偏置$b$合并到参数$\bold{w}$中，合并方法是在包含所有参数的矩阵中附加一列。
>
> 目标是将$||\bold{y - Xw}||^2$取得最小值，这在损失平面上只有一点，对应于整个区域的损失极小点，使$\frac{\partial ||\bold{y - Xw}||^2}{\partial \bold{w}} = 0$得到解析解：
> $$
> \bold{w}^{*} = \bold{(X^T X)^{-1}X^T y}
> $$

### 随机梯度下降算法

梯度下降最简单的用法是**计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度）。**

为了提高计算速率，通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做**小批量随机梯度下降（minibatch stochastic gradient descent）**。

在每次迭代中，首先随机抽样一个小批量B， 它是由固定数量的训练样本组成的。 然后，我们计算小批量的平均损失关于模型参数的导数（也可以称为梯度）。 最后，将梯度乘以一个预先确定的正数$\eta$，并从当前参数的值中减掉。
$$
(\bold{w},b) \gets (\bold{w},b)-\frac{\eta}{|B|}\sum_{i \in B}\partial_{(\bold{W},b)}l^{(i)}(\bold{w},b)
$$

> （1）初始化模型参数的值，如随机初始化； 
>
> （2）从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。

 $|B|$表示每个小批量中的样本数，也称为**批量大小（batch size）**。 $\eta$表示**学习率（learning rate）**。 批量大小和学习率的值通常是手动预先指定，而不是通过模型训练得到的。 这些**可以调整但不在训练过程中更新的参数**称为**超参数（hyperparameter）**。 

调参（hyperparameter tuning）是选择超参数的过程。 超参数通常是根据训练迭代结果来调整的， 而训练迭代结果是在独立的**验证数据集（validation dataset）**上评估得到的。

在训练了预先确定的若干迭代次数后（或者直到满足某些其他停止条件后）， 记录下模型参数的估计值，表示为$(\hat{\bold{w}},\hat{b})$。 但是，即使函数确实是线性的且无噪声，这些估计值也不会使损失函数真正地达到最小值。 因为算法会使得损失向最小值缓慢收敛，但却不能在有限的步数内非常精确地达到最小值。

线性回归恰好是一个在整个域中只有一个最小值的学习问题。 但是对像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。 

通常很少会去花费大力气寻找这样一组参数，使得在训练集上的损失达到最小。 事实上，更难做到的是找到一组参数，这组参数能够在从未见过的数据上实现较低的损失， 这一挑战被称为**泛化（generalization）**。

### 正态分布的观测噪声

正态分布概率密度函数：
$$
p(x) = \frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{1}{2\sigma^2}(x-\mu)^2)
$$
含有噪声的线性回归函数如下：
$$
y = \bold{w}^T\bold{x} + b + \epsilon
$$
因此，可以给出特定特征时得到预测值的概率（又称似然）：
$$
P(y|\bold{x}) = \frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{1}{2\sigma^2}(y -( \bold{w}^T\bold{x}+b)^2)
$$
为了得到最优的$(\bold{w},b)$，此时需要使用**最大似然估计法**，此时$P(\bold{y|X})$最大。

由于概率运算为乘法运算，使用负对数变为加法运算，即**最小化负对数**。
$$
-logP(\bold{y|X}) = \sum_{i=1}^n\frac{1}{2}log(2\pi\sigma^2)+\frac{1}{2\sigma^2}(y^{(i)} - (\bold{w}^T\bold{x}^{(i)}+b))^2
$$

### 线性神经网络

将线性回归模型描述为一个神经网络。 神经网络只显示连接模式（即每个输入如何连接到输出，省略权重和偏置）。

![NULL](picture_1.jpg)



在上图的神经网络中，输入维数（特征个数）为$d$，即**输入数（特征维度）**为$d$。输出只有一个，则**输出数**为1。

计算层数的时候不会考虑输入层，即**线性神经网络的层数为1。**

对于线性回归，每个输入和每个输出都是相连的，这种变换为**全连接层（fully-connected layer）**或**稠密层（dense layer）**。

## 2. 线性神经网络的实现

### 生成数据集

```python
def synthetic_data(w, b, num_examples):
    """生成数据集，满足正态分布
    Args:
        w : 权重向量
        b : 偏置
        num_examples : 样本的个数
    Returns:
        元组,第一个元素为特征张量,第二个元素为标签张量
    """
    X = torch.normal(0, 1, (num_examples, len(w)))
    # y = Wx + b
    y = torch.matmul(X,w)+b
    # 加入高斯噪声项
    y += torch.normal(0, 0.01, y.shape)
    return X, y.reshape((-1, 1))


# 生成模拟数据,自然生成列向量
tr_w = torch.tensor([2, -3.4])
tr_b = 4.2
# 生成特征和标签值
features, labels = synthetic_data(tr_w, tr_b, 1000)
# 生成散点图
d2l.set_figsize()
d2l.plt.scatter(features[:, (1)].detach().numpy(), labels.detach().numpy(), 1)
d2l.plt.show()
```

### 读取数据集

```python
def data_iter(batch_size, features, labels):
    """随机小批量读取数据集

    Args:
        batch_size: 样本批量大小
        features: 数据集特征矩阵
        labels: 标签向量
    Yields:
        随机样本特征子矩阵和标签子矩阵(生成器)
    """
    # batch:批
    num_examples = len(features)
    # 生成0 - num_examples 数据的索引列表
    indices = list(range(num_examples))     # indices:指标
    # 随机打乱列表（洗牌函数）
    random.shuffle(indices)                 # shuffle:洗牌
    # 将打乱后的数组分割，每段为batch_size，输出时使用生成器进行伪随机输出
    # 每次调用生成器时，batch_indices中的值会进行更新
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(
            indices[i:min(num_examples, i+batch_size)])
        yield features[batch_indices], labels[batch_indices]
```

### 初始化模型参数

```python
# 初始化模型参数,requires_grad表示表示需要计算梯度
w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
```

### 定义神经网络（模型）

```python
def linreg(X, w, b):
    """线性回归模型函数

    Args:
        X : 小批量样本特征矩阵 
        w : 权重向量 
        b : 偏置 
    Returns:
        求得的预测标签
    """
    return torch.matmul(X, w)+b
```

### 定义损失函数

```python
def squared_loss(y, y_hat):
    """平方损失函数

    Args:
        y : 实际标签向量
        y_hat : 预测标签向量
    Returns:
        平方损失
    """
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
```

### 定义随机梯度下降函数

```python
def sgd(params, lr, batch_size):
    """随机梯度下降函数

    Args:
        params : 神经网络参数
        lr : 学习率
        batch_size : 批量大小
    """
    # 禁用梯度计算
    with torch.no_grad():
        for param in params:
            # 进行梯度下降计算
            param -= lr * param.grad / batch_size
            # 清除梯度值
            param.grad.zero_()
```

### 进行训练

```python
# 训练
for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):   # 取小批量的样本
        l = loss(y, net(X, w, b))                          # 模型输出并计算损失函数
        l.sum().backward()                                 # 先求和，再求梯度
        sgd([w, b], lr, batch_size)                        # 随机梯度下降
        with torch.no_grad():
            train_l = loss(net(features, w, b), labels)                     # 将求出的参数代入原模型，计算损失函数
            print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
```

