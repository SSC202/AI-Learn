# AL_Learn 4_线性神经网络

## 1. 线性回归

**回归（regression）**是能为一个或多个自变量与因变量之间关系建模的一类方法。

### 线性回归的基本要素

1. 基本假设：

> - 自变量$x$和因变量$y$之间的关系是线性的。（$y$可以表示为$x$的加权和）（允许少量的噪声存在）
> - 假设任何噪声都比较正常，如噪声遵循正态分布。

2. 线性回归的基本要素

**训练数据集（training set）：**用于进行模型训练；

**样本（sample）**：训练数据集的每行数据；

**标签（label）**：试图预测的目标；

**特征（feature）**：预测依据的自变量。

3. 通常的，使用$n$表示数据集中的样本数，对于索引为$i$的样本，输入为$x^{(i)} = [x_1^{(i)},x_2^{(i)}]$，输出为$y^{(i)}$。

### 线性回归的基本模型

**线性假设指目标和特征的加权和。**
$$
\hat{y} = w_1x_1 + w_2x_2 + \ldots + w_mx_m + b
$$
式中$w_j$为**权重（weight）**，权重决定了每个特征对预测值的影响。$b$为**偏置（bias）**，指当所有特征都取值为0时，预测值应该为多少。

上式也被称之为输入特征的**仿射变换**，仿射变换的特点是通过加权和对特征进行**线性变换（linear transformation）**， 并通过偏置项来进行**平移（translation）**。

给定一个数据集，目标为寻找合适的权重和偏置，使得模型的预测尽可能准确。输出的预测值由输入特征通过线性模型的仿射变换决定，仿射变换由所选权重和偏置确定。

当输入包含$m$个特征时，令特征向量$\bold{x} \in \mathbb{R}^m $，权重向量$\bold{w} \in \mathbb{R}^b$，则
$$
\hat{y} = \bold{w}^T\bold{x} + b
$$
使用矩阵$\bold{X} \in \mathbb{R}^{n \times m}$可以表示每个样本。$\bold{X}$的每一行为一个样本，每一列为一个特征。则预测值：
$$
\bold{\hat{y}} = \bold{Xw}+b
$$
则目标为，给定训练数据特征$\bold{X}$和对应已知标签$\bold{y}$，找到一组权重向量$\bold{w}$和偏置$b$，使得预测误差尽可能小。

即使确信特征与标签的潜在关系是线性的， 也应加入一个噪声项来考虑观测误差带来的影响。

### 线性回归的损失函数

